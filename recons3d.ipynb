{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import imageio, sys, torch\n",
    "from PIL import Image\n",
    "sys.path.append(\"third_party/d2net/\")\n",
    "from lib.model_test import D2Net\n",
    "from lib.utils import preprocess_image\n",
    "from lib.pyramid import process_multiscale\n",
    "import cv2\n",
    "\n",
    "def d2net_features(im_path, max_size = 640):\n",
    "    # CUDA\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    # Creating CNN model\n",
    "    model = D2Net(\n",
    "        model_file=\"/home/niranjan/recons3d/third_party/d2net/models/d2_tf_no_phototourism.pth\",\n",
    "        use_relu=True,\n",
    "        use_cuda=use_cuda\n",
    "    )\n",
    "    image = imageio.imread(im_path)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image[:, :, np.newaxis]\n",
    "        image = np.repeat(image, 3, -1)\n",
    "\n",
    "    # TODO: switch to PIL.Image due to deprecation of scipy.misc.imresize.\n",
    "    resized_image = image\n",
    "    if max(resized_image.shape) > max_size:\n",
    "        print(max_size / max(resized_image.shape))\n",
    "        resized_image = cv2.resize(\n",
    "            resized_image, None, None,\n",
    "            max_size / max(resized_image.shape), max_size / max(resized_image.shape)\n",
    "        ).astype('float')\n",
    "    if sum(resized_image.shape[: 2]) > 2800:\n",
    "        resized_image = cv2.resize(\n",
    "            resized_image, None, None,\n",
    "            2800 / sum(resized_image.shape[: 2]), 2800 / sum(resized_image.shape[: 2])\n",
    "        ).astype('float')\n",
    "\n",
    "    fact_i = image.shape[0] / resized_image.shape[0]\n",
    "    fact_j = image.shape[1] / resized_image.shape[1]\n",
    "\n",
    "    input_image = preprocess_image(\n",
    "        resized_image,\n",
    "        preprocessing=\"caffe\"\n",
    "    )\n",
    "    with torch.no_grad():        \n",
    "        keypoints, scores, descriptors = process_multiscale(\n",
    "            torch.tensor(\n",
    "                input_image[np.newaxis, :, :, :].astype(np.float32),\n",
    "                device=device\n",
    "            ),\n",
    "            model,\n",
    "            scales=[1]\n",
    "        )\n",
    "\n",
    "    # Input image coordinates\n",
    "    keypoints[:, 0] *= fact_i\n",
    "    keypoints[:, 1] *= fact_j\n",
    "    # i, j -> u, v\n",
    "    keypoints = keypoints[:, [1, 0, 2]]\n",
    "    return {\"keypoints\":keypoints, \"descriptors\": descriptors, \"scores\":scores}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_256724/248916581.py:21: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(im_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43md2net_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/niranjan/recons3d/dataset/train/church/images/00002.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36md2net_features\u001b[0;34m(im_path, max_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m input_image \u001b[38;5;241m=\u001b[39m preprocess_image(\n\u001b[1;32m     44\u001b[0m     resized_image,\n\u001b[1;32m     45\u001b[0m     preprocessing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaffe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():        \n\u001b[0;32m---> 48\u001b[0m     keypoints, scores, descriptors \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_multiscale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Input image coordinates\u001b[39;00m\n\u001b[1;32m     58\u001b[0m keypoints[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m fact_i\n",
      "File \u001b[0;32m~/recons3d/third_party/d2net/lib/pyramid.py:86\u001b[0m, in \u001b[0;36mprocess_multiscale\u001b[0;34m(image, model, scales)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EmptyTensorError:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m fmap_pos \u001b[38;5;241m=\u001b[39m \u001b[43mfmap_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     87\u001b[0m fmap_keypoints \u001b[38;5;241m=\u001b[39m fmap_keypoints[:, ids]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ids\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "res = d2net_features(\"/home/niranjan/recons3d/dataset/train/church/images/00002.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectorfreesfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
